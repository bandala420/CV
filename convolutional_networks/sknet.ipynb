{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7ae78a-6c3f-4326-9b5b-49154dce7142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel Bandala @ apr 2022\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affbfaa4-6ced-40b7-842c-f3872ddbdcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SKConv(nn.Module):\n",
    "    def __init__(self, features, M=2, G=32, r=16, stride=1 ,L=32):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            features: input channel dimensionality.\n",
    "            M: the number of branchs.\n",
    "            G: num of convolution groups.\n",
    "            r: the ratio for compute d, the length of z.\n",
    "            stride: stride, default 1.\n",
    "            L: the minimum dim of the vector z in paper, default 32.\n",
    "        \"\"\"\n",
    "        super(SKConv, self).__init__()\n",
    "        d = max(int(features/r), L)\n",
    "        self.M = M\n",
    "        self.features = features\n",
    "        self.convs = nn.ModuleList([])\n",
    "        for i in range(M):\n",
    "            self.convs.append(nn.Sequential(\n",
    "                nn.Conv2d(features, features, kernel_size=3, stride=stride, padding=1+i, dilation=1+i, groups=G, bias=False),\n",
    "                nn.BatchNorm2d(features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Sequential(nn.Conv2d(features, d, kernel_size=1, stride=1, bias=False),\n",
    "                                nn.BatchNorm2d(d),\n",
    "                                nn.ReLU(inplace=True))\n",
    "        self.fcs = nn.ModuleList([])\n",
    "        for i in range(M):\n",
    "            self.fcs.append(\n",
    "                 nn.Conv2d(d, features, kernel_size=1, stride=1)\n",
    "            )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        feats = [conv(x) for conv in self.convs]      \n",
    "        feats = torch.cat(feats, dim=1)\n",
    "        feats = feats.view(batch_size, self.M, self.features, feats.shape[2], feats.shape[3])\n",
    "        \n",
    "        feats_U = torch.sum(feats, dim=1)\n",
    "        feats_S = self.gap(feats_U)\n",
    "        feats_Z = self.fc(feats_S)\n",
    "\n",
    "        attention_vectors = [fc(feats_Z) for fc in self.fcs]\n",
    "        attention_vectors = torch.cat(attention_vectors, dim=1)\n",
    "        attention_vectors = attention_vectors.view(batch_size, self.M, self.features, 1, 1)\n",
    "        attention_vectors = self.softmax(attention_vectors)\n",
    "        \n",
    "        feats_V = torch.sum(feats*attention_vectors, dim=1)\n",
    "        \n",
    "        return feats_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "589fd529-736f-4778-91e8-c3911bd0c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SKUnit(nn.Module):\n",
    "    def __init__(self, in_features, mid_features, out_features, M=2, G=32, r=16, stride=1, L=32):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            in_features: input channel dimensionality.\n",
    "            out_features: output channel dimensionality.\n",
    "            M: the number of branchs.\n",
    "            G: num of convolution groups.\n",
    "            r: the ratio for compute d, the length of z.\n",
    "            mid_features: the channle dim of the middle conv with stride not 1, default out_features/2.\n",
    "            stride: stride.\n",
    "            L: the minimum dim of the vector z in paper.\n",
    "        \"\"\"\n",
    "        super(SKUnit, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_features, mid_features, 1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        self.conv2_sk = SKConv(mid_features, M=M, G=G, r=r, stride=stride, L=L)\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(mid_features, out_features, 1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_features)\n",
    "            )\n",
    "        \n",
    "\n",
    "        if in_features == out_features: # when dim not change, input_features could be added diectly to out\n",
    "            self.shortcut = nn.Sequential()\n",
    "        else: # when dim not change, input_features should also change dim to be added to out\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_features, out_features, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_features)\n",
    "            )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2_sk(out)\n",
    "        out = self.conv3(out)\n",
    "        \n",
    "        return self.relu(out + self.shortcut(residual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe1a46c-89dd-4a1a-a60f-93f148b3c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SKNet(nn.Module):\n",
    "    def __init__(self, class_num, nums_block_list = [3, 4, 6, 3], strides_list = [1, 2, 2, 2]):\n",
    "        super(SKNet, self).__init__()\n",
    "        self.basic_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 7, 2, 3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(3,2,1)\n",
    "        \n",
    "        self.stage_1 = self._make_layer(64, 128, 256, nums_block=nums_block_list[0], stride=strides_list[0])\n",
    "        self.stage_2 = self._make_layer(256, 256, 512, nums_block=nums_block_list[1], stride=strides_list[1])\n",
    "        self.stage_3 = self._make_layer(512, 512, 1024, nums_block=nums_block_list[2], stride=strides_list[2])\n",
    "        self.stage_4 = self._make_layer(1024, 1024, 2048, nums_block=nums_block_list[3], stride=strides_list[3])\n",
    "     \n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(2048, class_num)\n",
    "        \n",
    "    def _make_layer(self, in_feats, mid_feats, out_feats, nums_block, stride=1):\n",
    "        layers=[SKUnit(in_feats, mid_feats, out_feats, stride=stride)]\n",
    "        for _ in range(1,nums_block):\n",
    "            layers.append(SKUnit(out_feats, mid_feats, out_feats))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fea = self.basic_conv(x)\n",
    "        fea = self.maxpool(fea)\n",
    "        fea = self.stage_1(fea)\n",
    "        fea = self.stage_2(fea)\n",
    "        fea = self.stage_3(fea)\n",
    "        fea = self.stage_4(fea)\n",
    "        fea = self.gap(fea)\n",
    "        fea = torch.squeeze(fea)\n",
    "        fea = self.classifier(fea)\n",
    "        return fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3913185-929c-48ed-a553-33779b0f8e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SKNet26(nums_class=1000):\n",
    "    return SKNet(nums_class, [2, 2, 2, 2])\n",
    "def SKNet50(nums_class=1000):\n",
    "    return SKNet(nums_class, [3, 4, 6, 3])\n",
    "def SKNet101(nums_class=1000):\n",
    "    return SKNet(nums_class, [3, 4, 23, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38633c17-f973-4b20-81f3-fbe67a992e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(8, 3, 224, 224)\n",
    "model = SKNet26()\n",
    "out = model(x)\n",
    "\n",
    "#flops, params = profile(model, (x, ))\n",
    "#flops, params = clever_format([flops, params], \"%.5f\")\n",
    "\n",
    "#print(flops, params)\n",
    "#print('out shape : {}'.format(out.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd2ef06-3721-4bb0-bf17-3ac0ca0aa635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1541, -0.5253,  0.4184,  ...,  0.2583,  0.7500,  0.3280],\n",
      "        [ 0.0745, -0.4862,  0.3181,  ...,  0.1068,  0.6685,  0.5190],\n",
      "        [ 0.2043, -0.5239,  0.3784,  ...,  0.2087,  0.6566,  0.3549],\n",
      "        ...,\n",
      "        [ 0.1209, -0.4830,  0.3647,  ...,  0.1825,  0.6942,  0.6221],\n",
      "        [ 0.0803, -0.6055,  0.3612,  ...,  0.0330,  0.7426,  0.5929],\n",
      "        [ 0.0332, -0.5381,  0.3228,  ...,  0.0465,  0.7293,  0.4008]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38191815-493e-4bf2-83c7-f90f5a765c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
