{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc75b2b9-310c-49f2-8c68-c9120d9bb97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea925d43-0aa6-46f6-9c0b-185fd155e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nn_softargmax = nn.Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c77f6d-6338-40d0-af0b-2a89a3f571b2",
   "metadata": {},
   "source": [
    "# Multi head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4861e172-3255-47f7-9b29-17d472e31cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, p, d_input=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        if d_input is None:\n",
    "            d_xq = d_xk = d_xv = d_model\n",
    "        else:\n",
    "            d_xq,d_xk,d_xv = d_input\n",
    "        # make sure the embedding dimension of model is a multiple of number of heads\n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.d_k = d_model//self.num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_xq, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_xk, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_xv, d_model, bias=False)\n",
    "        \n",
    "        # outputs of all sub-layers\n",
    "        self.W_h = nn.Linear(d_model,d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self,Q,K,V):\n",
    "        batch_size = Q.size(0)\n",
    "        k_length = K.size(-2)\n",
    "        # scaling by d_k so the softmaxarg dont saturate\n",
    "        Q = Q/np.sqrt(self.d_k)\n",
    "        scores = torch.matmul(Q,K.transpose(2,3))\n",
    "        A = nn_softargmax(dim=-1)(scores)\n",
    "        # get the weigthed average of the values\n",
    "        H = torch.matmul(A,V)\n",
    "        return H,A\n",
    "    \n",
    "    def split_heads(self,x,batch_size):\n",
    "        \"\"\" Split the last dimension into (heads X depth)\n",
    "        return after transpose to put in shape \"\"\"\n",
    "        return x.view(batch_size,-1,self.num_heads,self.d_k).transpose(1,2)\n",
    "    \n",
    "    def group_heads(self,x,batch_size):\n",
    "        \"\"\" Combine the heads to get batch_size X seq_length X num_heads X d_k \"\"\"\n",
    "        return x.transpose(1,2).contiguous().view(batch_size,-1,self.num_heads*self.d_k)\n",
    "    \n",
    "    def forward(self,X_q,X_k,X_v):\n",
    "        batch_size,seq_length,dim = X_q.size()\n",
    "        # after transforming split into num_heads\n",
    "        Q = self.split_heads(self.W_q(X_q), batch_size)\n",
    "        K = self.split_heads(self.W_k(X_k), batch_size)\n",
    "        V = self.split_heads(self.W_v(X_v), batch_size)\n",
    "        # calcultate the attention weights for each head\n",
    "        H_cat,A = self.scaled_dot_product_attention(Q,K,V)\n",
    "        # put all heads back together by concat\n",
    "        H_cat = self.group_heads(H_cat,batch_size)\n",
    "        # final linear layer\n",
    "        H = self.W_h(H_cat)\n",
    "        return H,A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57849f65-f302-411e-b4c5-b42dbf95a7b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Some sanity checks\n",
    "\n",
    "To check self attention works - if the query matches with one of the key values, it should have all the attention focused there with the value returned as the value at that index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cad6638-b446-4da5-af7c-cd5cdec425d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create multi head attention class\n",
    "temp_mha = MultiHeadAttention(d_model=512,num_heads=8,p=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b616efb1-a0c6-4578-ad98-711eb6f18998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(Q,K,V):\n",
    "    temp_out,temp_attn = temp_mha.scaled_dot_product_attention(Q,K,V)\n",
    "    print(\"Attention weights are: \", temp_attn.squeeze())\n",
    "    print(\"Output is: \", temp_out.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b692a13-e103-41e5-bcb7-2e60fac637ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:  tensor([3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06])\n",
      "Output is:  tensor([1.0004e+01, 4.0993e-05, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "test_K = torch.tensor(\n",
    "    [[10,0,0],\n",
    "     [ 0,10,0],\n",
    "     [ 0,0,10],\n",
    "     [ 0,0,10]]\n",
    ").float()[None,None]\n",
    "test_V = torch.tensor(\n",
    "    [[   1,0,0],\n",
    "     [  10,0,0],\n",
    "     [ 100,5,0],\n",
    "     [1000,6,0]]\n",
    ").float()[None,None]\n",
    "test_Q = torch.tensor(\n",
    "    [[0,10,0]]\n",
    ").float()[None,None]\n",
    "\n",
    "print_out(test_Q,test_K,test_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "767fc0c4-ddf1-40d7-9277-2d2ff17c518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:  tensor([1.8633e-06, 1.8633e-06, 5.0000e-01, 5.0000e-01])\n",
      "Output is:  tensor([549.9979,   5.5000,   0.0000])\n"
     ]
    }
   ],
   "source": [
    "test_Q = torch.tensor(\n",
    "    [[0,0,10]]\n",
    ").float()\n",
    "print_out(test_Q,test_K,test_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb3dbe69-aec3-48a2-888f-0b5f91205be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:  tensor([[1.8633e-06, 1.8633e-06, 5.0000e-01, 5.0000e-01],\n",
      "        [3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06],\n",
      "        [5.0000e-01, 5.0000e-01, 1.8633e-06, 1.8633e-06]])\n",
      "Output is:  tensor([[5.5000e+02, 5.5000e+00, 0.0000e+00],\n",
      "        [1.0004e+01, 4.0993e-05, 0.0000e+00],\n",
      "        [5.5020e+00, 2.0497e-05, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "test_Q = torch.tensor([[0,0,10],[0,10,0],[10,10,0]]).float()\n",
    "print_out(test_Q,test_K,test_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c3fc3-7b5a-441a-bf4b-8b6fbfd2bc5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1D convolution with kernel_size=1\n",
    "\n",
    "This is basically an MLP with one hidden layer and ReLU activation applied to each and every element in the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40eb232d-2a9b-43d0-b168-d10345ab54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,d_model,hidden_dim,p):\n",
    "        super().__init__()\n",
    "        self.k1convL1 = nn.Linear(d_model,hidden_dim)\n",
    "        self.k1convL2 = nn.Linear(hidden_dim,d_model)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.k1convL1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.k1convL2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0afe85-92ca-40d7-a59f-055844547a8b",
   "metadata": {},
   "source": [
    "# Transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cfd6904-caf7-461a-b47f-ec79aa46ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,num_heads,conv_hidden_dim,p=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model,num_heads,p)\n",
    "        self.cnn = CNN(d_model,conv_hidden_dim,p)\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model,eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model,eps=1e-6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # multi-head attention\n",
    "        attn_output,_ = self.mha(x,x,x)\n",
    "        # layer norm after adding the residual connection\n",
    "        out1 = self.layernorm1(x+attn_output)\n",
    "        # feed forward\n",
    "        cnn_output = self.cnn(out1)\n",
    "        # second layer norm after adding residual connection\n",
    "        out2 = self.layernorm2(out1+cnn_output)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8238e-997f-408f-a4cf-b42ed2dd900c",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Blocks of N Encoder Layers + Positional encoding + Input embedding\n",
    "\n",
    "Self attention by itself does not have any recurrence or convolutions so to make it sensitive to position we must provide additional position encodings. These are calculated as follows\n",
    "$$ E(p,2i) = sin(p/10000^{2i/d}) $$\n",
    "$$ E(p,2i+1) = cos(p/10000^{2i/d}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64a06d87-c7ec-4152-acec-675f67b7e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sin_embeddings(nb_p,dim,E):\n",
    "    theta = np.array([\n",
    "        [p/np.power(10000,2*(j//2)/dim) for j in range(dim)] for p in range(nb_p)\n",
    "    ])\n",
    "    E[:,0::2] = torch.FloatTensor(np.sin(theta[:,0::2]))\n",
    "    E[:,1::2] = torch.FloatTensor(np.cos(theta[:,1::2]))\n",
    "    E.detach_()\n",
    "    E.requires_grad = False\n",
    "    E = E.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7496ea52-96d0-4c18-b9c8-c680ab190ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self,d_model,vocab_size,max_position_embeddings,p):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size,d_model,padding_idx=1)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings,d_model)\n",
    "        create_sin_embeddings(\n",
    "            nb_p=max_position_embeddings,\n",
    "            dim=d_model,\n",
    "            E=self.position_embeddings.weight\n",
    "        )\n",
    "        self.LayerNorm = nn.LayerNorm(d_model,eps=1e-12)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length,dtype=torch.long,device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        # get word embedding for each input id\n",
    "        word_embeddings = self.word_embeddings(input_ids)\n",
    "        # get position embeddings for each position id\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        # add them both\n",
    "        embeddings = word_embeddings+position_embeddings\n",
    "        # layer norm\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        # return result\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61fd62a0-9cda-438e-9b2b-bb9b68aecd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,num_layers,d_model,num_heads,ff_hidden_dim,input_vocab_size,maximum_position_encoding,p=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.embeddings = Embeddings(d_model,input_vocab_size,maximum_position_encoding,p)\n",
    "        # multiple layers of encoders\n",
    "        self.enc_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.enc_layers.append(EncoderLayer(d_model,num_heads,ff_hidden_dim,p))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff46e28-4c30-48cd-857a-93f6ac97652a",
   "metadata": {},
   "source": [
    "## Use transformer layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea9f1c2c-9232-4f65-b2ee-c44813c64cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c6cbf-1de3-4b97-96c1-398b7abfd3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
